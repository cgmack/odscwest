{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##Tokenizing Text\n",
    "Returning the the Gutenberg, get Alice in Wonderland as text and as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import gutenberg text\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve Alice In Wonderland text\n",
    "from nltk.corpus import gutenberg\n",
    "alice_raw = gutenberg.raw(fileids=['carroll-alice.txt']).lower()\n",
    "alice_words = nltk.wordpunct_tokenize(alice_raw) #could also use word_tokenize(text)\n",
    "alice_vocab = set(alice_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##Inspect the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Looking at the first 100 words\n",
    "print alice_words[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many words are in this book? \n",
    "print \"Raw: \", len(alice_raw)\n",
    "print \"Words: \", len(alice_words)\n",
    "print \"Vacabulary: \", len(alice_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print alice_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist1 = FreqDist(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1.tabulate(10) #the top 20 most frequently seen words in this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1['rabbit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "fdist1.plot(40,cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1['very']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "fdist1.plot(200,cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for token in fdist1:\n",
    "    if fdist1[token] < 60 and fdist1[token] > 50:\n",
    "        print fdist1[token], '\\t', token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_bigrams = nltk.bigrams(alice_words)\n",
    "print alice_bigrams[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist2 = nltk.FreqDist(alice_bigrams)\n",
    "stopwords = ['!','.',',',';',\"'\",\",'\",\"!'\",\"?'\",\"*\",\":\",'a','-','\"','$','the','and','i','was','my','a','for','be','to','is','it','she','you']\n",
    "for k,v in fdist2.items():\n",
    "    if k[0] not in stopwords and k[1] not in stopwords and v > 15:\n",
    "        print v,k\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using NLTK's Text Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice = nltk.Text(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice.count('turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice.concordance('turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "alice.dispersion_plot(['alice','rabbit','turtle','queen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.text import TextCollection\n",
    "gutenberg = TextCollection(nltk.corpus.gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import text1, text2, text3, text4, text5, text6, text7, text8, text9\n",
    "mytexts = TextCollection([text1, text2, text3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gutenberg.tf_idf(\"rabbit\",text6) #Monty Python and the Holy Grail\n",
    "print gutenberg.tf_idf(\"rabbit\",text3) #The Book of Genesis\n",
    "print gutenberg.tf_idf(\"rabbit\",alice) #Alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gutenberg.count(\"rabbit\")\n",
    "print alice.count(\"rabbit\")\n",
    "print text6.count(\"rabbit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tokenizing Other Langauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import genesis \n",
    "fileids = genesis.fileids()\n",
    "\n",
    "#tokenize at Sentence level. Print first sentence of each file.\n",
    "for file_name in fileids:\n",
    "    raw = genesis.raw(fileids=[file_name])\n",
    "    sents = nltk.sent_tokenize(raw)\n",
    "    print \"------------------------\"\n",
    "    print sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
